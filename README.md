# desafio-dio-pipeline-azure
Este projeto demonstra a constru√ß√£o de um pipeline de engenharia de dados utilizando o Azure Data Factory. A proposta foi integrar uma base de dados MySQL local com o Azure Data Lake Storage Gen2, permitindo a extra√ß√£o e o armazenamento dos dados no formato .csv

# üîÑ Pipeline de Engenharia de Dados com Azure Data Factory

Este projeto faz parte do bootcamp **Microsoft AI for Tech - Azure** promovido pela **DIO.me**. O desafio consistiu em criar um pipeline no **Azure Data Factory** que extrai dados de uma tabela local em **MySQL**, transforma se necess√°rio, e carrega os dados como um arquivo `.csv` no **Azure Data Lake Storage Gen2**.

---

## üéØ Objetivo do Projeto

Demonstrar a cria√ß√£o de um pipeline de engenharia de dados usando ferramentas da Microsoft Azure, com foco em:
- Conex√£o com banco de dados local (MySQL)
- Integra√ß√£o com Azure Data Factory
- Armazenamento de dados no Azure Data Lake (Gen2) no formato `.csv`

---

## üõ†Ô∏è Tecnologias Utilizadas

- MySQL (local)
- Azure Data Factory
- Azure Storage Account (Gen2)
- Integration Runtime (Self-hosted)
- MySQL Workbench (interface de acesso)
- Azure Portal

---

## üóÇÔ∏è Estrutura do Projeto

1. **Banco de Dados Local**
   - Tabela criada no MySQL com dados de teste
   - Exemplo:
     ```sql
     CREATE TABLE employees (
     Emp_id INT PRIMARY KEY,
     F_name VARCHAR(50),
     L_name VARCHAR(50),
     ssn CHAR(6),
     B_date (DATE),
     sex CHAR(1),
     adress VARCHAR(100),
     job_id INT,
     salary DECIMAL(10,2),
     manager_id INT,
     dep_id INT
);
     ```

2. **Azure Data Factory**
   - Criado pipeline com os seguintes componentes:
     - Linked Service para MySQL
     - Linked Service para Azure Blob Storage (Gen2)
     - Dataset de origem (MySQL)
     - Dataset de destino (.csv)
     - Atividade de c√≥pia (Copy Data)

3. **Self-hosted Integration Runtime**
   - Instalado e configurado localmente
   - Autenticado com o Azure Data Factory

---

## üì∏ Prints do Projeto

### üéõÔ∏è Azure Data Factory - Pipeline
![print_pipeline](https://github.com/Rodrigo-Antonio-Silva/desafio-dio-pipeline-azure/blob/0cd8192aa7c6825e1c4d73d2ca079f3ce73757d8/Captura%20de%20tela%202025-05-20%20090137.png)

### üóÉÔ∏è Tabela no MySQL
![print_mysql](https://github.com/Rodrigo-Antonio-Silva/desafio-dio-pipeline-azure/blob/8896a6176c629288d77ef96ad4f2454ae8a09df4/Captura%20de%20tela%202025-05-20%20085830.png)

### üìÇ Arquivo .CSV no Storage
![print_csv](https://github.com/Rodrigo-Antonio-Silva/desafio-dio-pipeline-azure/blob/0cd8192aa7c6825e1c4d73d2ca079f3ce73757d8/Captura%20de%20tela%202025-05-20%20090314.png)

---

## üí° Aprendizados e Insights

- A conex√£o entre banco de dados local e o Azure exige a instala√ß√£o e configura√ß√£o do **Integration Runtime**.
- A cria√ß√£o de pipelines no Data Factory √© visual e eficiente para processos ETL.
- O armazenamento em Gen2 oferece escalabilidade e integra√ß√£o com outras solu√ß√µes da Azure.

---

## üöÄ Pr√≥ximos Passos

- Automatizar esse processo com trigger por tempo.
- Adicionar transforma√ß√£o dos dados (Data Flow).
- Armazenar os dados em formatos otimizados como Parquet ou Delta.

---

## üìé Refer√™ncia

- [Documenta√ß√£o oficial do Azure Data Factory](https://learn.microsoft.com/pt-br/azure/data-factory/)

---

## ‚úÖ Status

‚úÖ Desafio Conclu√≠do com sucesso.
